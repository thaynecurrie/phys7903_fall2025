{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6b2c5a",
   "metadata": {},
   "source": [
    "# _Python for Scientific Data Analysis_\n",
    "\n",
    "# SciPy\n",
    "\n",
    "## Section 3: Statistics\n",
    "\n",
    "_“There are three kinds of lies: Lies, Damned Lies, and Statistics”_\n",
    "\n",
    "  Mark Twain (or was it Benjamin Disraeli?)\n",
    "\n",
    "The ``scipy.stats`` subpackage contains different a large number of functions dedicated to the latter: frequentists statistics, probability, distributions, correlation, functions, different statistical tests, etc.   Want to generate a distribution with a known functional form?  this subpackage has it?  Is one distribution different than another?  How different? This subpackage has it. \n",
    "\n",
    " In previous generations, a lot of these topics were covered in the venerable \"Numerical Recipes\" textbook (er, phonebook) for code in C and FORTRAN.   Your instructor and his advisor (and his advisor before him, most likely) learned numerical techniques from this source.   We are no longer in the dark ages of dial-up Internet and cellphones the size of books, but this text (written *before* such an era) IS the authority on this topic, in my opinion.  And the value of that text to highlight what is important even in Python remains.  \n",
    " \n",
    " Therefore, some of my discussion will basically look like a \"_Numerical Recipes in SciPy_\" (someone should write this textbook).\n",
    " \n",
    " The key topics we want to cover are as follows:\n",
    " \n",
    " * _Statistical Properties of Data_ - You have some data, how do you describe it?\n",
    " * _Distributions of Data_ - You have some data, how is the data-as-an-aggregate intelligible?  What patterns do it fit into?\n",
    " * _Statistical Tests of Data_ - You have some data, how sure are you that it differs from some other data, how sure are you that something you think is interesting is actually significant vs noise, etc.\n",
    "\n",
    " Similar to before, our key start-up commands are to import relevant libraries: numpy, matplotlib, and the stats subpackage of SciPy:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "372ea164",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import scipy\n",
    "\n",
    "#and because this is a Jupyter notebook\n",
    "%matplotlib inline\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize']=[8,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188c75b",
   "metadata": {},
   "source": [
    "### Properties of Data\n",
    "\n",
    "We have some data.  How would you describe it?   Well, let's then start with some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b741de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[10, 14, 11, 7, 9.5], [8, 9, 17, 14.5, 12],\n",
    "              [15, 7, 11.5, 10, 10.5], [11, 11, 9, 12, 14]])\n",
    "              \n",
    "print(A.shape) #this is a (4,5) array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758eb67",
   "metadata": {},
   "source": [
    "#### _Basic Properties_\n",
    "\n",
    "Key properties of the data include the mean, median, standard deviation, and variance.  \n",
    "\n",
    "* mean -- The mean of an array is $\\Sigma_{i} A_{i} /N$, where $N$ is the number of elements of $A$.    Note, that if you call the program without any axis keyword, then the mean is calculated over the entire array.  Whereas the axis keyword specifies that the mean is calculated over a specific dimension.  E.g. ``mean=scipy.stats.tmean(A,axis=1)``.\n",
    "\n",
    "* median -- the median of an array $x$ of length $n$ equals  $x_{(n+1)/2}$ if $n$ is odd and $0.5\\times(x_{n/2} + x_{(n/2)+1} )$ if $n$ is even.   \n",
    "\n",
    "* variance -- By \"variance\" here we usually mean the \"average of the squared deviations from the mean\": $S^{2}$ = $\\Sigma_{i} (x_{i}-\\bar{x})^{2}$/$N$, where $N$ equals the total population size.   This is also known as the _population variance_.     There is also the _sample variance_, where $S^{2}$ = $\\Sigma_{i} (x_{i}-\\bar{x})^{2}$/($n-1$) for $n$ samples.   \n",
    "\n",
    " The best way to think about the latter is that we are picking out a sample of $n$ from the larger population of $N$.   As such, we have an additional correction factor (the \"Bessell correction\", which is responsible for the $n-1$ in the denominator.   You can also think of this as a \"degree of freedom\": i.e. you estimate the variance from a random sample of $n$ independent measurements, so the dof equals $n$ minus the number of parameters estimated (i.e. the sample mean).\n",
    "\n",
    "* standard deviation -- The standard deviation of a random variable, $\\sigma$, is equal to the square root of its variance.  e.g. for a population standard deviation, \n",
    " $\\sigma$ = $\\sqrt{\\Sigma_{i} (x_{i}-\\bar{x})^{2}/N}$.   Similarly, the sample standard deviation includes a $n-1$ factor in the denomintor, not $N$:\n",
    " $\\sigma$ = $\\sqrt{\\Sigma_{i} (x_{i}-\\bar{x})^{2}/(n-1)}$\n",
    "\n",
    "\n",
    "We can call these from SciPy as follows:\n",
    "\n",
    "* ``mean=scipy.stats.tmean(A,axis=axis)``\n",
    "* ``median=scipy.ndimage.median(A)`` . I.e. Within SciPy, the function to compute the median is (confusingly) found in the ``ndimage`` subpackage, not stats.  \n",
    "* ``variance = scipy.stats.tvar(A,ddof=ddof)``.\n",
    "* ``standard_deviation = scipy.stats.tstd(A,ddof=ddof,axis=axis)``\n",
    "\n",
    "Note here that the weirdness with the function calls.   The \"mean\" call is understandable (though why is it not in stats?) but median lives in another subpackage.  Similarly, the variance and standard deviation are actually computed from the _trimmed [variance/standard deviation]_ functions.  What this does is allow you to select a subset of the sample range in computing the variance or standard deviation, though of course if you do not put anything for this keyword, then the full range is considered.\n",
    "\n",
    "For these and probably a bunch of other reasons, most often you want to use equivalent NumPy functions for these operations.   Here I list the functions with the major keywords and their default values.   So in NumPy ...\n",
    "\n",
    "* ``mean=np.mean(A,axis=None,ddof=0)``\n",
    "* ``median=np.median(A,axis=None)``\n",
    "* ``variance=np.var(A,axis=None,ddof=0)``\n",
    "* ``standard_deviation = np.std(A,axis=None,ddof=0)``\n",
    "\n",
    "``axis=None`` says \"compute ___ over entire array\", whereas ``axis=[not None/some number]`` says compute over a specific axis.   Also, the ddof keyword refers to \"delta degrees of freedom\": the divisor in the calculations use $n-ddof$.  By default, ddof=0 (i.e. this is a maximum likelihood estimate of the variance for normally distributed variables).\n",
    "\n",
    "Anyway, this all seems rather silly (who cares so long as you know which version you are using, right?).  Except that the default assumptions about what is _actually_ being computed varies from programming language to language and package to package.  \n",
    "\n",
    "E.g. take the array ``[2. , 3. , 4. , 2.1, 2.2, 3.3]``\n",
    "\n",
    "Here's what you get if you just ask NumPy what the mean, standard deviation and variance are without any keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c4ed983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy \n",
      "\n",
      "2.766666666666667\n",
      "0.7318166133366716\n",
      "0.5355555555555555\n",
      "\n",
      "Scipy \n",
      "\n",
      "2.766666666666667\n",
      "0.8016649341630621\n",
      "0.6426666666666666\n"
     ]
    }
   ],
   "source": [
    "#NumPy\n",
    "\n",
    "print('NumPy \\n')\n",
    "a=np.array([2. , 3. , 4. , 2.1, 2.2, 3.3])\n",
    "print(np.mean(a))\n",
    "#2.766666666666667\n",
    "\n",
    "print(np.std(a))\n",
    "#0.7318166133366716\n",
    "\n",
    "print(np.var(a))\n",
    "#0.5355555555555555\n",
    "\n",
    "\n",
    "#and SciPy\n",
    "print('')\n",
    "print('Scipy \\n')\n",
    "#SciPy\n",
    "\n",
    "print(scipy.stats.tmean(a))\n",
    "#2.766666666666667\n",
    "\n",
    "print(scipy.stats.tstd(a))\n",
    "#0.8016649341630621   #uh oh!\n",
    "\n",
    "print(scipy.stats.tvar(a))\n",
    "#0.6426666666666666  #oh no!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1871ab1-d727-4755-b16b-f0be3b80b426",
   "metadata": {},
   "source": [
    "Okay, maybe this is just some weirdness with SciPy vs NumPy ... nope!   Here are the equivalent commands in IDL\n",
    "\n",
    "```\n",
    "IDL> print,mean(a)\n",
    "#2.76667\n",
    "\n",
    "IDL> print,stdev(a)\n",
    "#0.801665 #SciPy-like\n",
    "\n",
    "IDL> print,variance(a)\n",
    "#0.64266670 #SciPy-like again\n",
    "```\n",
    "\n",
    "\n",
    "The solution to reconcile these answers is to understand what exactly NumPy and SciPy are calculating.   E.g. NumPy's standard deviation and variance are population values (i.e. big \"N\" in the denominator), while SciPy's are sample standard deviations and variances (little \"n\" in the denominator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b5279a9-7b68-438a-9d10-44add85a52b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8016649341630621\n",
      "0.5355555555555555\n"
     ]
    }
   ],
   "source": [
    "#making NumPy's standard deviation agree with SciPy's\n",
    "print(np.std(a,ddof=1))\n",
    "\n",
    "#making SciPy's trimmed variance agree with NumPy's\n",
    "print(scipy.stats.tvar(a,ddof=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5927e42c-8e97-402b-aa6e-0753d47736c2",
   "metadata": {},
   "source": [
    "So yeah, the NumPy implementations of these basic statistical properties are pretty user friendly ... but be very careful that you understand **exactly** what you are computing.\n",
    "\n",
    "_Dealing with NaNs_\n",
    "\n",
    "Now, if you have NaNs in your array these simple NumPy functions are probably going to crash when you try to compute the mean, median, variance or standard deviation.  Thankfully, NumPy has thought ahead and created functions that compute these statistical values _while ignoring_ NaNs:\n",
    "\n",
    "``np.nanmean``, ``np.nanmedian``, ``np.nanvar``,``np.nanstd``.\n",
    "\n",
    "#### _More Advanced Properties_\n",
    "\n",
    "The above statistics can be done with either NumPy (recommended, provided you understand caveats) or SciPy (works but \"Rube-Goldberg machine\"-ish?).  For more advanced characteristics, SciPy -- in particular ``scipy.stats`` -- becomes more useful and is the preferred option.\n",
    "\n",
    "Some more complicated properties include\n",
    "\n",
    "* skew -- ``scipy.stats.skew`` -- Gives the relative weight to the right or left tail of a distribution.  For normally distributed data, the skewness should be about zero. For unimodal continuous distributions, a skewness value greater than zero means that there is more weight in the right tail of the distribution.  The sample skewness is computed as the Fisher-Pearson coefficient of skewness: $g_{1}$=$m_{3}$/$m_{2}^{1.5}$, where $m_{i} = \\Sigma_{n=1}^{N} (x[n]-\\bar{x})^{i}$/$N$\n",
    "\n",
    "* kurtosis -- ``scipy.stats.kurtosis`` -- is the measure of the \"tailed-ness\" of a given distribution (i.e. how often outliers occur).   It equals the \"fourth central moment\" divided by the square of the variance (i.e. $\\mu_{4}$/$S^{2}$= $\\mu_{4}$/$\\sigma^{4}$).   For a normal distribution the value of the kurtosis is 0.\n",
    "\n",
    "* interquartile range -- ``scipy.stats.iqr``-- The interquartile range is the difference between the 75th and 25th percentile of the data.   It is an outlier-resistant measure of the dispersion of data.  \n",
    "\n",
    "   Now this is great but what if you actually want the values for the 75th or 25th percentile?  I'm sure there is a scipy function to do this, but the simpler route is to just use NumPy.  E.g. ``np.nanpercentile([array],25)`` gives the 25th percentile value of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d6b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb7253-bb3f-4782-8a09-6f2fb06f19d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
